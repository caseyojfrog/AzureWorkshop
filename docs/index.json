[
{
	"uri": "https://jefferyfry.github.io/awsworkshop/50_devops_cloud/1_continuous_integration_and_delivery.html",
	"title": "Continuous Integration and Delivery",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/1_fork_workshop_repos.html",
	"title": "Fork the Lab Repositories",
	"tags": [],
	"description": "",
	"content": "To get started, you will need to fork two GitHub repositories. These repositories contain the files that are needed for the lab.\n https://github.com/shimib/Horae - Contains our JFrog pipelines for configuring our Artifactory server and building our Java webapp and NPM user interface.\n https://github.com/shimib/project-examples - Contains our webapp code which consists of the Java webapp and NPM user interface.\n   Navigate to each of these repositories GitHub using the links above and fork them to your account. In the top-right corner of the GitHub repository page, click Fork. Do this for both repositories. In your terminal, clone each of the new forks to your local git directory.\n$ git clone https://github.com/[username]/Horae.git$ git clone https://github.com/[username]/project-examples  In the next sections, you will make changes to the files in these repositories.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/",
	"title": "JFrog Modernization Workshop",
	"tags": [],
	"description": "",
	"content": " DevOps Modernization Workshop Welcome In this workshop you will learn about the JFrog Platform and how to leverage Artifactory, XRay and Pipelines for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud on AWS.\nLearning Objectives  Understand the roles of Artifactory, XRay and Pipelines in your software delivery life cycle (SDLC). Use Local, Remote and Virtual Repositories in Artifactory. Publish and promote Build Info. Scan your artifacts and builds for security vulnerabilities.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/50_devops_cloud/10_binary_repository_management.html",
	"title": "Binary Repository Management",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/10_configure_github_integration.html",
	"title": "Configure the GitHub Integration",
	"tags": [],
	"description": "",
	"content": "In order for JFrog Pipelines, to get access to the code in your new repositories, we must first set up a Pipelines GitHub integration. This allows Pipelines to authenticate and get access to your GitHub repositories. To do this, we create a GitHub Personal Access Token with the correct permissions.\n Go to your GitHub Personal Access Tokens settings page. Click on Generate new token. Provide a name for the token. Configure the token for the following scopes.\n* repo (all) * admin:repo_hook (read, write) * admin:public_key (read, write) Click Generate token.\n Copy the token.\n Go to your JFrog Platform instance at https://[server name].jfrog.io. Refer to your JFrog Free Subscription Activation email if needed.\n Login to your JFrog Platform instance with your credentials.  Once logged into the environment, you will be presented with the landing page.  On the left sidebar menu, select Pipelines ► Integrations.  Click on Add an Integration.\n Give this integration the name GitHub.\n For the Integration Type, choose GitHub.\n Paste your GitHub Personal Access Token into the Token field.  Click Create.\n  You have created a GitHub Integration that allows JFrog Pipelines to access your GitHub repositories.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/10_prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Please set up these items before beginning.\n Access to a JFrog Platform instance with Artifactory, Xray and Pipelines - Get your own JFrog Platform instance with the JFrog Platform Cloud Free Tier in just a few minutes. A GitHub account for accessing and modifying workshop code - Create a GitHub account with these official instructions.  \n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/20_configure_artifactory_integration.html",
	"title": "Configure the Artifactory Integration",
	"tags": [],
	"description": "",
	"content": "Similar to the GitHub Integration, in the following steps you will configure an Artifactory Integration that allows JFrog Pipelines to also access your Artifactory repositories in order to publish your artifacts and build info. You will do this by creating an API key.\nArtifactory offers a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.   In your JFrog Platform instance, go your profile and select Edit Profile. Enter your password and click Unlock to edit the profile. In the Authentication Settings section, click the gear icon to generate an API key.  Copy the API key. Go back to Integrations, select Pipelines ► Integrations.  Click on Add an Integration. Give this integration the name Artifactory. For the Integration Type, choose Artifactory. Enter your JFrog Platform instance URL https://[server name].jfrog.io/artifactory for the url. Enter your username for the User. Paste your Artifactory API Key into the API Key field.  Click Create.  You have created an Artifactory Integration that allows JFrog Pipelines to access your Artifactory repositories. At this point, you should see the Artifactory and the GitHub Integrations in the Integrations list.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/50_devops_cloud/20_dev_sec_ops.html",
	"title": "DevSecOps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/30_configure_initialization_pipeline.html",
	"title": "Configure the Initialization Pipeline",
	"tags": [],
	"description": "",
	"content": "Next, we will update the lab pipelines to add your new GitHub and Artifactory integrations. In previous steps, you forked and cloned the lab repositories. We will modify the initialization pipeline in your forked Horae repository to add these integrations.\n In your local git directory, open Horae/pipelines/base_init.yml in an editor. Update the resources section of the file to use your forked repositories. Change the path to use your username.\nresources: - name: demo_gitRepo type: GitRepo configuration: path: [your_Github_username]/Horae \u0026lt;\u0026lt;\u0026lt;--- CHANGE HERE gitProvider: GitHub - name: gitRepo_code type: GitRepo configuration: path: [your_Github_username]/project-examples \u0026lt;\u0026lt;\u0026lt;--- CHANGE HERE gitProvider: GitHub branches: include: eplus-v2-orbitera  Save your changes.\n In your terminal, git add, commit and push your changes.\n$ git add . $ git commit -m \u0026#39;Updated repository paths.\u0026#39; $ git push Go to https://github.com/[username]/Horae/blob/master/pipelines/base_init.yml and verify your changes were made and pushed to your GitHub repository.\n  We are now ready to add and execute your pipelines with JFrog Pipelines.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/50_devops_cloud/30_unified_solution.html",
	"title": "Unified Platform",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/40_configure_pipeline_source.html",
	"title": "Configure the Pipeline Source",
	"tags": [],
	"description": "",
	"content": "In these next steps, we will add the previous pipelines as a JFrog Pipelines source. This will allow JFrog Pipelines to executes these pipelines automatically whenever there is a commit or manually as needed.\nA Pipeline Source represents a source control repository (such as GitHub or BitBucket) where Pipelines definition files can be found. A pipeline source connects to the repository through an integration.   In your JFrog Platform instance, go to Pipelines ► Pipeline Sources.  Click on Add Pipeline Source. Choose Single Branch. This is for repositories that only have one branch like master. For the Integration choose the GitHub Integration that you created previously named GitHub. For the Repository Full Name, specify your Horae repository name in the form [username]/Horae. Leave the Branch as master. For the Pipeline Config Filter, specify pipelines/base__.*yml. Click Create.   It will take a few moments for JFrog Pipelines to sync the pipelines from your new Pipelines Source. During this time, JFrog Pipelines will load and process the pipelines for syntax, resources and dependencies. When complete, you should see a Success status. 9. Click on Logs on the right to view more details on the sync process. 10. Go back to Pipelines ► My Pipelines and you will see your added pipelines. With your pipelines added, we are now ready to execute the pipelines, initialize our environment and build our artifacts!\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/50_devops_cloud.html",
	"title": "DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/50_execute_initialization_pipeline.html",
	"title": "Execute the Initialization Pipeline",
	"tags": [],
	"description": "",
	"content": "The first pipeline that we will execute will initialize our environment. This pipeline will create users, groups, permissions, repositories, Xray policies and watches, Xray indexes and access federation. This prepares our JFrog Platform instance to run our gradle and npm build pipelines.\nThis pipeline initializes the JFrog Platform for the next build pipelines by creating the necessary users, repositories, permissions and Xray configuration. It does this by using the JFrog Platform REST APIs. This is another way that you can manage and monitor the JFrog Platform.   Go to Pipelines ► My Pipelines.  Click on the init_environment pipeline in the Pipelines List. Click on the first step and further click on the trigger step icon to execute this pipeline. A run will appear and it will take a few moments for JFrog Pipelines to allocate resources to execute the pipeline.  If the execution results in an error, click on the run to view the logs. Make any changes to the pipeline or integrations to correct any issues and then execute again.  The run will show a success status when it completes without errors.   We are now ready to build our first artifacts for our application.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/60_execute_gradle_build_pipeline.html",
	"title": "Execute the Gradle Build Pipeline",
	"tags": [],
	"description": "",
	"content": "Our first build pipeline is a Gradle pipeline that builds our Java web app. This pipeline uses a GradleBuild native Pipelines step to compile and build the code. DockerBuild and DockerPush native steps are used to build a Docker image and push it to Artifactory. It then scans the build using the XrayScan native step. Then it pushes the produced artifacts to the staging repository in Artifactory along with all build information by using the PromoteBuild native step.\nA Step is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline. Steps take Inputs in the form of Integrations or Resources, execute tasks that perform the operations necessary and then produce Outputs. These Outputs can become Inputs to other steps and so on forming a dependency-based, event-driven pipeline.   Go to Pipelines ► My Pipelines.  Click on the gradle_build pipeline in the Pipelines List. Click on the View YAML icon to the right to view the pipeline steps discussed above.  Click on the Resources tab to view the details for all the resources that are used in our pipeline. We specify the relevant input and output resources such as Git repositories, build infos and file specs. These are referenced in our pipeline. Click on the Pipeline tab. The beginning of the YAML states our pipeline name along with one read-only global environment variable.\nname: gradle_build configuration: environmentVariables: readOnly: Version: 1.1.$run_number View the first step, gradle_bld_app. This is the GradleBuild native step that executes Gradle to build our Java webapp. The following occurs in this step:\n We specify the Gradle command to execute, the source location, and the location for some configuration files. By using the autoPublishBuildInfo parameter, we ensure the build information will be kept in Artifactory. We specify our gitRepo_code GitRepo resource as an input resource, our Artifactory integration and gradleBuildInfo as an output resource.\n- name: gradle_bld_app type: GradleBuild configuration: runtime: type: image image: custom: # THe Docker image is used to support a java 8 image for the gradle build name: docker.bintray.io/jfrog/pipelines-u18java tag: \u0026#34;8\u0026#34; gradleCommand: clean artifactoryPublish -b build.gradle --stacktrace sourceLocation: tutorial/step1-create-gradle-app configFileLocation: . configFileName: jfrog-gradle.yml autoPublishBuildInfo: true inputResources: - name: gitRepo_code integrations: - name: Artifactory outputResources: - name: gradleBuildInfo  The second step, Gradle_docker_build, executes a docker build. It does the following:\n It will use the Docker file from the relevant location to generate the Docker image with a specific name and tag based on the value of the environment variables that were configured in the onStart. It will use the gitRepo_code GitRepo resource as an input resource to locate the Dockerfile. It will use the BuildInfo input resource from the previous step. It will also use an additional input resource for downloading the Gradle-created artifact into the workspace of the running step, to be used as part of the Docker build\n- name: Gradle_docker_build type: DockerBuild configuration: affinityGroup: dock1 dockerFileLocation: tutorial/step1-create-gradle-app dockerFileName: Dockerfile dockerImageName: ${Fullimagename} dockerImageTag: ${Version} inputSteps: - name: gradle_bld_app inputResources: - name: gitRepo_code trigger: false - name: gradle_fileSpec trigger: false - name: gradleBuildInfo trigger: false integrations: - name: Artifactory execution: onStart: # export artifactory ip from internal build env var to tag the docker image - export domain=$(echo ${int_Artifactory_url} | awk -F[/:] \u0026#39;{print $4}\u0026#39; ) - export Fullimagename=\u0026#34;${domain}/docker-demo/gradle-app\u0026#34;  The next step will push the Docker image to the target Docker repository in Artifactory.\n - name: Gradle_docker_push type: DockerPush configuration: affinityGroup: dock1 targetRepository: docker-demo autoPublishBuildInfo: true integrations: - name: Artifactory inputSteps: - name: Gradle_docker_build outputResources: - name: docker_gradleBuild_Info The following step scan the docker image produced in the preceding step using Xray for security vulnerabilities and license compliance. By default, a failed Xray scan will result in a failure of the step and the pipeline.\n - name: Gradle_docker_scan type: XrayScan configuration: inputResources: - name: docker_gradleBuild_Info trigger: true outputResources: - name: scanned_gradle_dockerBuild_Info  The last step will promote the build after passing the previous Xray scan.\n - name: gradle_docker_promote type: PromoteBuild configuration: targetRepository: docker-demo includeDependencies: true status: Passed comment: Artifact passed Xray Scan copy: false inputResources: - name: scanned_gradle_dockerBuild_Info trigger: true outputResources: - name: final_docker_gradleBuild_Info Close the VIEW YAML window.\n Click on the first step and further click on the trigger step icon to execute this pipeline. A run will appear and it will take a few moments for JFrog Pipelines to allocate resources to execute the pipeline. It will take several minutes for this pipeline to run (~10 minutes).  Click on the active run to view the progress of the pipeline.  Use the pulldown to select the various steps and view the logs for each step.  When the run completes successfully, go to Artifactory ► Builds to view your Gradle build.  Click on gradle_build. This will show you a list of the last builds for your Gradle build.  Click on your last promoted build with a Status of Passed.\n Click on any of the modules to view the module artifacts and dependencies.  For any artifact or dependency, click on the icon for Show In Tree to view it in the repository.  Go back to the Build view Artifactory ► Builds and explore the other tabs.\n The Environment tab shows your system and environment variables.  The Xray Data tab shows any security vulnerabilities and license compliance issues.  The Diff tab allows you to compare this build to previous builds and shows the differences.   This Gradle build pipeline provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory, Pipelines and Xray. You were able to execute a pipeline, monitor the progress and examine its results. Let\u0026rsquo;s move on to the next pipeline to explore more features.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/60_jfrog_platform_overview.html",
	"title": "JFrog Platform Overview",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute software. It provides DevOps with the tools needed to create, manage and deploy software with ease. These tools cover everything from binary management, artifact maturity, security and vulnerability protection, CI/CD orchestration, release management, analytics and distribution.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Mission Control and Insight is your DevOps dashboard solution for managing multiple services of Artifactory, Xray, Edge and Distribution.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.\nAll of these JFrog Platform components are designed and developed to work together out-of-the-box with minimal configuration. Management and monitoring of your software delivery lifecycle from build to distribution is accessible though a central, unified user interface. The JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow.\n "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/70_execute_npm_build_pipeline.html",
	"title": "Execute the NPM Build Pipeline",
	"tags": [],
	"description": "",
	"content": "Our second build pipeline is a NPM pipeline that builds our user interface that connects to our previously built Gradle Java webapp. This pipeline uses a NpmBuild native Pipelines step build the user interface components. Then it uses a generic Bash step to package the components. Next, it uses NpmBuild to publish the components. Finally, similar to the Gradle build, DockerBuild, DockerPush, XrayScan and PromoteBuild are used to promote a final docker image to the staging repository.\nSteps are executed on build nodes. Dynamic build node pools are spun up and down on-demand by Pipelines from a cloud or Kubernetes service. This can help scale operations, and help manage costs by not incurring cloud service charges to run idle nodes. Static build node pools can also be used and are persistently available.   Go to Pipelines ► My Pipelines.  Click on the npm_build pipeline in the Pipelines List. Click on the View YAML icon to the right to view the pipeline steps discussed above.  Click on the Resources tab to view the details for all the resources that are used in our pipeline. Click on the Pipeline tab. The first step npm_prep is an NpmBuild step. This prepares the NPM environment for building. Additionally, the following occurs:\n The NPM repository is specified with repositoryName. Our Artifactory integration specifies our Artifactory server and inputResources is the same source code location. onStart is used to execute bash commands to set our NPM version prior to executing the step. Steps have additional lifecycle callbacks: onExecute, onSuccess, onFailure and onComplete.\n- name: npm_prep type: NpmBuild configuration: npmArgs: --no-progress --no-audit repositoryName: npm-demo sourceLocation: tutorial/step2-create-ui-pkg integrations: - name: Artifactory inputResources: - name: gitRepo_code execution: onStart: - pushd ${res_gitRepo_code_resourcePath}/tutorial/step2-create-ui-pkg - npm version ${Version} --no-git-tag-version - popd  Next, we have a Bash step, npm_compile. Bash steps are very powerful, because they allows you to execute practically any commands. The purpose of this step is to compile our NPM package. In this step, we do the following in onStart:\n restore_run_files is a Pipeline Utility Function copies the current run state or context to a location to compile. npm install installs our packages. add_run_files adds our temporary state or context back to the initial run state.\n- name: npm_compile type: Bash configuration: inputSteps: - name: npm_prep integrations: - name: Artifactory execution: onStart: - export tempStateLocation=\u0026#34;$step_tmp_dir/npmSourceState\u0026#34; - restore_run_files npmBuildInputGitRepo $tempStateLocation - pushd $tempStateLocation/tutorial/step2-create-ui-pkg - npm install shelljs - npm install - add_run_files $tempStateLocation/. npmBuildInputGitRepo - popd  The next step, npm_publish, uses NpmPublish to publish a package to the Artifactory repository npm-demo.\n- name: npm_publish type: NpmPublish configuration: repositoryName: npm-demo integrations: - name: Artifactory inputSteps: - name: npm_compile execution: onStart: - export inputNpmBuildStepName=\u0026#34;npm_prep\u0026#34; The remaining steps are the same as what you saw in the Gradle build. DockerBuild, DockerPush, XrayScan and PromoteBuild are used to build, scan, publish and promote our docker image with our NPM user interface to a staging repository.\n Close the VIEW YAML window.\n Click on the first step and further click on the trigger step icon to execute this pipeline. It will take several minutes for this pipeline to run (~10 minutes).  When the run is finished successfully, switch to the Builds view in Artifactory. Go to Artifactory ► Builds.\n Click on npm_build in the list.\n Then click on your most recent build.  In the Published Modules tab, view the set of artifacts and dependencies for your build.  In the Xray Data tab, view the security and license violations.   This NPM build pipeline provided another of example of how JFrog Pipelines, Artifactory and Xray can automate your software delivery. You explored new steps for NPM and learned how to use the flexible Bash step.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab.html",
	"title": "JFrog Devops Hands-On Lab",
	"tags": [],
	"description": "",
	"content": "In this tutorial we’ll build a containerized microservice with an example Java and NPM two-tier web application which requires third party dependencies and Docker images used for deployment. The Pipelines provided in this tutorial include two pre-configured pipelines that will provide you the experience in building and publishing a multi-stage web application.\n gradle-build creates a Java web app which displays some images.\n npm-build builds a NPM user interface that connects to the Java web app.\n  Throughout all of the lab pipelines, all artifacts are pushed to Artifactory. Artifacts in Artifactory are annotated with metadata, especially in the case of builds which come with exhaustive metadata that facilitate full traceability. This, for example, prevents the deletion of dependencies which are needed by a build. We’ll see this later on in the tutorial when we use the Tree Browser in Artifactory to view one of the artifacts produced in our build.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_jfrog_devops_hands-on_lab/80_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, we demonstrated using the JFrog Platform to create CI/CD pipelines to build an application, manage the artifacts, scan the artifacts for security vulnerabilities and license compliance, and publish the artifacts to a staging repository. Now that you have some basic understanding of the JFrog Platform, we encourage you to use it with your existing build tools, like Maven, Gradle, Ivy and Ant and work with packaging systems of other development platforms, like NuGet, RubyGems or NPM.\nIn addition to JFrog Pipelines, you can leverage Artifactory with other CI servers such as Jenkins, Hudson, TeamCity, and Bamboo. Artifactory is also a great artifact repository for your DevOps needs, since it natively supports managing Docker images, Vagrant images, as well as RPM and Debian package managers.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/999_additional_resources.html",
	"title": "Additional Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all of the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]